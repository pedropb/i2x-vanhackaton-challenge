# i2x Vanhackaton challenge 2017

`unconcatenate_words.py` is a tool to unconcatenate strings (as "iwanttogotoberlin" -> "i want to go to berlin") using the [text8 dataset](http://mattmahoney.net/dc/textdata) as a corpus dictionary.

**WARNING:** This project is deprecated. I found this [reference](http://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb) which describes the problem of word segmentation in detail, and I will be implementing a solution on [another repo](https://github.com/pedropb/i2x-word-segmentation-challenge)

**Table of Contents**
<!-- toc -->

- [Problem Description](#problem-description)
- [Installation](#installation)
- [Usage](#usage)
- [Examples](#examples)
- [Remarks](#remarks)

<!-- tocstop -->

## Problem Description

Oh, no!

You have just completed a lengthy document when you have an unfortunate Find/Replace mishap. You have accidentally removed all spaces, punctuation, and capitalisation in the document.

A sentence like "I reset the computer. It still didn't boot!" would become iresetthecomputeritstilldidntboot".

You figure that you can add back in the punctation and capitalisation later, once you get the individual words properly separated. Most of the words will be in a dictionary, but some strings, like proper names, will not.


Given a dictionary (a list of words), design an algorithm to find the optimal way of "unconcatenating" a sequence of words. In this case, "optimal" is defined to be the parsing which minimizes the number of unrecognized sequences of characters.


For example, the string "jesslookedjustliketimherbrother" would be optimally parsed as "JESS looked just like TIM her brother". This parsing has seven unrecognized characters, which we have capitalised for clarity.


## Installation

```
$ git clone https://github.com/pedropb/i2x-challenge
$ pip install -r requirements.txt
```

## Usage

```
$ cd i2x-challenge
$ ./unconcatenate_words.py -h
usage: unconcatenate_words.py [-h] [-t FREQUENCY_THRESHOLD] concatenated_file

Unconcatenate words from a string based on a given dictionary. Words not
present on the dictionary will be written in UPPERCASE on the output and will
count towards the number of unrecognized characters. Outputs: the
unconcatenated string and the number of unrecongized characters.

positional arguments:
  concatenated_file     File containing the string with all words concatenated

optional arguments:
  -h, --help            show this help message and exit
  -t FREQUENCY_THRESHOLD, --frequency-threshold FREQUENCY_THRESHOLD
                        Frequency threshold (in %) for considering words from
                        the corpus dictionary (Default: 0.001). For example,
                        words that appear on the corpus dictionary less than
                        0.001% won't be used.
```


## Examples

<details>
<summary><pre>$ ./unconcatenate_words.py examples/text1</pre></summary>
Found and verified text8.zip
Concatenated text: YouhavejustcompletedalengthydocumentwhenyouhaveanunfortunateFindReplacemishapYouhaveaccidentallyremovedallspacespunctuationandcapitalisationinthedocument

Unconcatenated text: you  have  just  completed  a  lengthy  document  when  you  have  an  unfortunate  find  replace  mishap  you  have  accidentally  removed  all  spaces  punctuation  and  capitalisation  in  the  document

Unrecognized characters:  0
</details>

<details>
<summary><pre>$ ./unconcatenate_words.py examples/text2</pre></summary>
Found and verified text8.zip
Concatenated text: YoufigurethatyoucanaddbackinthepunctationandcapitalisationlateronceyougettheindividualwordsproperlyseparatedMostofthewordswillbeinadictionarybutsomestringslikepropernameswillnot

Unconcatenated text: you  figure  that  you  cana  dd  back  int  hep  unc T ation  and  capitalisation  later  once  you  get  the  individual  words  properly  separated  most  oft  he  words  will  bein  a  dictionary  but  some  strings  like  proper  names  wil  lnot
Unrecognized characters:  1
</details>

## Remarks

- This tool implements a simple greedy algorithm solutions.
- It doesn't always output the optimal solution, but it comes close to what a human would expect.
- I didn't use Machine Learning for this project, because:

    1. There was no labeled dataset provided to train a model upon.
    2. Building a labeled dataset for this challenge would require precious time.
    3. Besides building the labeled dataset:
        - it would be required to build a Sequence-to-sequence model
        - try multiple architectures
        - train it and tune it
    4. Finally, I think the result generated by this solution is close to what I would get using a Sequence-to-sequence model, taking into consideration, the time I would need to spend to tune its hyperparameters and arquitecture.
    
- On a final note, there is also a hyperparameter for this algorithmic model, which is `-t frequency-threshold`. The default value is 0.001, which means that words that appear less than 0.001% in the corpus dictionary won't be used for matching in the string. In Layman's terms, it means that increasing the threshold will result in common words matching, while decreasing it, will result in rare words matching. This parameter must be between 100 and 0 (not inclusive).
